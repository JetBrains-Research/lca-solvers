# Iteration parameters
max_iters: 80_000
valid_freq: 100
gradient_accumulation_steps: 2
micro_batch_size: 2

# AdamW optimizer
learning_rate: 6.0e-4
beta_1: 0.9
beta_2: 0.95
weight_decay: 0.01
max_grad_norm: 1

# Cosine lr scheduler with warmup
decay_lr: True
warmup_iters: 2000
lr_decay_iters: 60_000
min_lr: 6.0e-5

# Train-validation split
valid_size: 128
upper_bound_per_repo: 5
random_seed_split: 1337

# DataLoader
shuffle: True
drop_last: False
num_workers: 4
random_seed_dl: 1337
