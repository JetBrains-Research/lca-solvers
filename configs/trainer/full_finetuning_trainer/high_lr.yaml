# Iteration parameters
max_iters: 600
valid_freq: 25
checkpointing_freq: 25
gradient_accumulation_steps: 64
micro_batch_size: 1

# AdamW optimizer
learning_rate: 7.0e-5
beta_1: 0.9
beta_2: 0.999
weight_decay: 0.01
max_grad_norm: 2

# Cosine lr scheduler with warmup
warmup_iters: 25
lr_decay_iters: 600
min_lr: 1.0e-7

# Metrics
train_metrics: [
  cross_entropy,
  detached_cross_entropy,
  completion_cross_entropy,
  context_cross_entropy,
  full_cross_entropy,
  commited_cross_entropy,
  common_cross_entropy,
  infile_cross_entropy,
  inproject_cross_entropy,
  non_informative_cross_entropy,
  random_cross_entropy,
  epoch,
  learning_rate,
]
train_ema_alpha: 0.01
valid_metrics: [
  cross_entropy,
  detached_cross_entropy,
  completion_cross_entropy,
  context_cross_entropy,
  full_cross_entropy,
  commited_cross_entropy,
  common_cross_entropy,
  infile_cross_entropy,
  inproject_cross_entropy,
  non_informative_cross_entropy,
  random_cross_entropy,
  epoch,
]
valid_ema_alpha: null

# DataLoader
shuffle: True
drop_last: False
num_workers: 8
prefetch_factor: 8
random_seed: 1337

# Floating point
fp32_matmul_precision: high
