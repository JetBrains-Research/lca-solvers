{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29162485-f6f0-4f9f-b915-80849ced191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057c461-e100-4837-bae4-f74ed8f404a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745822f-ccca-4d51-9106-a02f073caeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n",
    "# https://anirbansen2709.medium.com/finetuning-llms-using-lora-77fb02cbbc48\n",
    "# https://github.com/microsoft/LoRA/tree/main/examples/NLG\n",
    "# https://github.com/huggingface/peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9693c61-4234-461e-8605-71f17a5e184d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45a570-2ce1-4b86-9947-7e980756cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3497d4-3b2c-4852-a6c3-324c562669e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !DS_BUILD_OPS=1 pip install --no-cache-dir deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492737f-600f-45cb-9d5f-3c6114777ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepspeed.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c51b3-58cf-4edc-89d7-dde67472ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78d9ac-098b-4282-84ab-9bdfc1c2880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e68fd74c-3920-4a9e-a7c7-bbaa39d4b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c16632c-2dfe-4933-8580-d369c7f1510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/blatova/lca-solvers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998d928f-6d9e-478c-a618-02c6dd31777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import finetuning_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e79bd5-5a29-44b8-a05b-60b580c419c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mekaterina-blatova\u001b[0m (\u001b[33morg12345\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47e07da-af61-4ba4-bdce-d8f2f73464ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d9ac8e84e54b91b582f7c89f1955ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b80559-0c8d-4ac7-8947-6b879e7c3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 20:06:04,853] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "import os\n",
    "import deepspeed\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#CHANGED_LAST\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/blatova/lca-solvers/')\n",
    "\n",
    "from data_filters.repo_snapshot_filter_stack import SnapshotFilterStack\n",
    "from data_classes.datapoint_composed import DatapointComposed\n",
    "from data_loading.raw_train_dataset_loading import DataLoaderTrainRaw\n",
    "from data_loading.composer_train_dataset_loading import DataLoaderTrainComposed\n",
    "from context_composers.context_composer_path_distance import ContextComposerPathDistance\n",
    "from data_classes.datapoint_composed import DatapointComposed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, RepoCard\n",
    "\n",
    "punctuation = string.punctuation\n",
    "punctuation += \"–—‘’“”…\"\n",
    "pattern = re.compile(r'^[a-zA-Z0-9\\s' + re.escape(punctuation) + r']*$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a529ffd-27f3-4b90-bd4f-0b1bc19bb05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=6,7 jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896c8b75-cc0b-4f4f-94a9-da04eba6681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 11 20:06:06 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   30C    P8              14W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:22:00.0 Off |                  Off |\n",
      "|  0%   30C    P8              16W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090        Off | 00000000:41:00.0 Off |                  Off |\n",
      "|  0%   29C    P8              14W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090        Off | 00000000:61:00.0 Off |                  Off |\n",
      "|  0%   29C    P8              12W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        Off | 00000000:81:00.0 Off |                  Off |\n",
      "|  0%   30C    P8              14W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        Off | 00000000:A1:00.0 Off |                  Off |\n",
      "|  0%   29C    P8              10W / 450W |  23928MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 4090        Off | 00000000:C1:00.0 Off |                  Off |\n",
      "|  0%   29C    P8              27W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce RTX 4090        Off | 00000000:E1:00.0 Off |                  Off |\n",
      "|  0%   29C    P8              20W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    5   N/A  N/A   2639809      C   python                                    23910MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89bb198-8b97-4cb9-a45b-6d5901dc2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICES_TO_TRAIN=[3]\n",
    "WORLD_SIZE= len(DEVICES_TO_TRAIN)\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(x) for x in DEVICES_TO_TRAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7221ae-f592-47eb-9105-6dd8932feb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a3b1a4-2ac5-467f-a2a8-71cef0e3594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 1694536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edc37799-cea7-4400-9d45-164b56daa178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b80783-2c12-4807-af35-9e190295c57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0c01e2e-f5c2-4abd-90f1-688d3bbec02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3072f435-a05c-4b07-9a58-4c4756155e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# loader = DataLoader(cache_dir=\"/home/blatova/cache_dir\")\n",
    "# loader = DataLoaderTrainComposed(\n",
    "#         hf_path='JetBrains-Research/context-py-train',\n",
    "#         hf_config=None,\n",
    "#         cache_dir='/mnt/data2/shared-data/lca/hf_cache2/',\n",
    "#     )\n",
    "composer = ContextComposerPathDistance()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", \n",
    "                                             trust_remote_code=True, torch_dtype=torch.bfloat16, device_map = \"auto\", attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca3d7c-3ffc-4f29-845a-827af87f3902",
   "metadata": {},
   "source": [
    "### Lora parameters\n",
    "\n",
    "https://medium.com/@manyi.yim/more-about-loraconfig-from-peft-581cf54643db\n",
    "\n",
    "https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\n",
    "\n",
    "https://abvijaykumar.medium.com/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-2-d8e23877ac6f\n",
    "\n",
    "AdaMix Lora: https://arxiv.org/pdf/2210.17451v1\n",
    "\n",
    "Lora-based mixture of experts: https://arxiv.org/pdf/2404.15159\n",
    "\n",
    "I believe the modules you've mentioned: gate_proj, down_proj, up_proj, q_proj, v_proj, k_proj, and o_proj, are all linear layers. So, they should be included in the list for LoRA finetuning to get the best results, that compare to full finetuning.\n",
    "\n",
    "This is based on the QLoRA paper: \"We find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers is required to match full finetuning performance.\"\n",
    "\n",
    "To check which layers are linear, you can load the model with the transformers library in Python and then just print(model). The layers to look for will be labeled Linear or Linear4bit if you load the model in 4-bit. While I can't say this is the only way, it's how I've always done it.\n",
    "\n",
    "For QLoRA finetuning, I've used my own custom script, so I wrote a function that returns a list to simplify the \"finding layers to finetune\" process. However, keep in mind, this function will work properly only if the model is loaded in 4-bit using the transformers library:\n",
    "\n",
    "```def find_target_modules(model):\n",
    "    # Initialize a Set to Store Unique Layers\n",
    "    unique_layers = set()\n",
    "    \n",
    "    # Iterate Over All Named Modules in the Model\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the Module Type Contains 'Linear4bit'\n",
    "        if \"Linear4bit\" in str(type(module)):\n",
    "            # Extract the Type of the Layer\n",
    "            layer_type = name.split('.')[-1]\n",
    "            \n",
    "            # Add the Layer Type to the Set of Unique Layers\n",
    "            unique_layers.add(layer_type)\n",
    "\n",
    "    # Return the Set of Unique Layers Converted to a List\n",
    "    return list(unique_layers)```\n",
    "From: https://www.reddit.com/r/LocalLLaMA/comments/15sgg4m/what_modules_should_i_target_when_training_using/\n",
    "\n",
    "```import re\n",
    "model_modules = str(model.modules)\n",
    "pattern = r'\\((\\w+)\\): Linear'\n",
    "linear_layer_names = re.findall(pattern, model_modules)\n",
    "\n",
    "names = []\n",
    "# Print the names of the Linear layers\n",
    "for name in linear_layer_names:\n",
    "    names.append(name)\n",
    "target_modules = list(set(names))```\n",
    "\n",
    "\n",
    "Better result about layers and parameters fine-tuning:\n",
    "\n",
    "https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms : All linear layers\n",
    "\n",
    "***Other parameters:***\n",
    "\n",
    "***Optimizer: Commonly used AdamW***\n",
    "\n",
    "r=16 => alpha = 16 (That’s why the authors set α to the first r and do not tune it. The default of α is 8.)\n",
    "\n",
    "***r=32 => alpha = 32***\n",
    "\n",
    "Dropout rate: \n",
    "\n",
    "https://ar5iv.labs.arxiv.org/html/2403.00812\n",
    "\n",
    "https://arxiv.org/pdf/2403.00812\n",
    "\n",
    "https://arxiv.org/pdf/2404.09610\n",
    "\n",
    "Hence, we only introduce the dropping in the latter half of layers in decoder-only models and the apparent performance improvement emerges again.\n",
    "\n",
    "***Dropout rate: Start without dropout? Usually used 0.1 ***\n",
    "\n",
    "bias: ???\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c972d79f-9980-435e-9519-3424c29dfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGED\n",
    "# ds = load_dataset('JetBrains-Research/context-py-train', 'path_distance_relevant', cache_dir = '/mnt/data2/shared-data/lca/hf_cache2/')\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=32, lora_alpha=32, lora_dropout=0.1, target_modules='all-linear'\n",
    ")\n",
    "# peft_config = LoraConfig(\n",
    "#             lora_alpha=args.lora_alpha,\n",
    "#             lora_dropout=args.lora_dropout,\n",
    "#             r=args.lora_r,\n",
    "#             bias=\"none\",\n",
    "#             task_type=TaskType.CAUSAL_LM,\n",
    "#             target_modules=\"all-linear\"\n",
    "#         )\n",
    "# model_lora = get_peft_model(model, peft_config)\n",
    "# model_lora_no_parallel = torch.nn.DistributedDataParallel(model_lora, DEVICES_TO_TRAIN)\n",
    "# model_lora = deepspeed.init_inference(model_lora_no_parallel, mp_size=len(DEVICES_TO_TRAIN)) \n",
    "# model.print_trainable_parameters()\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5addb2b9-1eb1-4d6b-8d70-3a54a77e68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUFFIX='lr4e-5_old_scheduler_with_warmup_3_epochs_relevant'\n",
    "def save_checkpoint(checkpoint_num, model, val_loss_whole_input , val_loss_completion, validation_dataset, suffix=SUFFIX):\n",
    "    #CHANGED\n",
    "    repo_name = f\"model_{suffix}_v{checkpoint_num}\"\n",
    "    model.push_to_hub(repo_name)\n",
    "    model_description = f\"\"\"\n",
    "    \n",
    "    ## Evaluation results\n",
    "    \n",
    "    Validation loss on the whole input: {val_loss_whole_input}\n",
    "\n",
    "    Validation loss on completion: {val_loss_completion}\n",
    "    \n",
    "    \"\"\"\n",
    "    model_card = RepoCard(model_description)\n",
    "    model_card.push_to_hub(f\"ekaterina-blatova-jb/{repo_name}\")\n",
    "    # if checkpoint_num ==0:\n",
    "    #     validation_dataset.push_to_hub(f\"val_dataset_{suffix}\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a15feb79-ad0b-42a0-b8ee-be43c2a4b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_val_dataset(val_indices_list):\n",
    "    return dataset['train'].select(val_indices_list)\n",
    "\n",
    "def write_val_dataset(std_val_dataset_version, val_dataset, validation_repos, val_indices_list):\n",
    "    torch.save(val_dataset, f'std_val_dataset/std_val_dataset_{std_val_dataset_version}.pt')    \n",
    "    with open(f\"std_val_dataset/std_val_dataset_{std_val_dataset_version}_repos.pkl\", \"wb\") as repos: \n",
    "        pickle.dump(validation_repos, repos)\n",
    "    with open(f\"std_val_dataset/std_val_dataset_{std_val_dataset_version}_indices.pkl\", \"wb\") as indices: \n",
    "        pickle.dump(val_indices_list, indices)\n",
    "    val_dataset.push_to_hub(f\"val_dataset_std_{std_val_dataset_version}\")\n",
    "    \n",
    "\n",
    "def read_val_dataset(std_val_dataset_version):\n",
    "    with open(f\"std_val_dataset/std_val_dataset_{std_val_dataset_version}_repos.pkl\", \"rb\") as repos: \n",
    "        validation_repos = pickle.load(repos)\n",
    "    with open(f\"std_val_dataset/std_val_dataset_{std_val_dataset_version}_indices.pkl\", \"rb\") as indices: \n",
    "        val_indices_list = pickle.load(indices)\n",
    "    val_dataset = get_standard_val_dataset(val_indices_list)\n",
    "    return val_dataset, validation_repos, val_indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be7a53-59f4-41a8-98c8-cf7ee1340f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f994641d-649c-4a40-883f-b41ae385e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lora.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d446b349-0a02-4949-ba77-28ed6fb172e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6000 tokens => OOM\n",
    "ACCUM_STEPS_NUM = 64\n",
    "# VALIDATION_STEPS = 16\n",
    "VALIDATION_PERIOD = 10\n",
    "# CONTEXT_MAX_LEN_CHARS = 5000\n",
    "CONTEXT_MAX_LEN_TOKENS = 2000\n",
    "EMA_PERIOD = 40\n",
    "EMA_ALPHA = 2/(EMA_PERIOD+1)\n",
    "EPOCHS=3\n",
    "WARMUP_STEPS = 200\n",
    "ADAM_MAX_LR = 4e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44a60c4b-eca3-46bf-b60f-5cbc0bbf9b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2894736842105263"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(275/950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b4f50b7-62db-4df1-b3c1-08b725024a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T_max = 275 is for 1 epoch\n",
    "lambda1 = lambda x:  x/ WARMUP_STEPS if x < WARMUP_STEPS else 1\n",
    "optimizer = AdamW(model.parameters(), lr=ADAM_MAX_LR, weight_decay=0.01) #added weight decay\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose='deprecated')\n",
    "scheduler_warmup = LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "scheduler_cosine = CosineAnnealingLR(optimizer, T_max=(208*EPOCHS-WARMUP_STEPS), eta_min=1e-8) #added scheduler\n",
    "\n",
    "\n",
    "#CHANGED_LAST\n",
    "# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=200, num_training_steps = 730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf451d8-5740-4530-8136-1fdfc7034ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9946cc54-0eda-484d-82f1-c4b1fec0e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_input_sequence(dp: DatapointComposed, max_len_tokens: int, tokenizer, context_ratio: float = 0.75, \n",
    "                           cut_context=True) -> str:\n",
    "    #here we will probably get a little bit less than a real number of tokens\n",
    "    approx_max_len_chars = max_len_tokens * 3\n",
    "    \n",
    "    if not -1e8 < context_ratio < 1. + 1e8:\n",
    "        raise ValueError('context_ratio must be between 0 and 1')\n",
    "\n",
    "    context = dp.context[0]\n",
    "    completion = dp.completion[0]\n",
    "    context = '\\n'.join([line for line in context.split('\\n') if re.match(pattern, line)])\n",
    "    completion = '\\n'.join([line for line in completion.split('\\n') if re.match(pattern, line)])\n",
    "\n",
    "    length_context = int(approx_max_len_chars * context_ratio) + 1\n",
    "    length_completion = int(approx_max_len_chars * (1 - context_ratio)) + 1\n",
    "\n",
    "    compl_trim_idx = completion.rfind('\\n', 0, length_completion)\n",
    "    context_trim_idx = context.find('\\n', len(context)-length_context, len(context))\n",
    "\n",
    "    if compl_trim_idx > 0:\n",
    "        # print(\"First cut of completion\")\n",
    "        completion_trimmed = completion[:compl_trim_idx]\n",
    "    else:\n",
    "        # print(\"Second cut of completion\")\n",
    "        completion_trimmed = completion[:length_completion]\n",
    "    if context_trim_idx > 0:\n",
    "        # print(\"First cut of context\")\n",
    "        context_trimmed = context[1+context_trim_idx:]\n",
    "    else:\n",
    "        # print(\"Second cut of context\")\n",
    "        context_trimmed = context[-length_context:]\n",
    "    \n",
    "    context_len = tokenizer(context_trimmed, return_tensors=\"pt\")['input_ids'].shape[1]\n",
    "    completion_len = tokenizer(completion_trimmed, return_tensors=\"pt\")['input_ids'].shape[1]\n",
    "    res_input = tokenizer(context_trimmed + completion_trimmed, return_tensors=\"pt\")\n",
    "    \n",
    "    if completion_len>=length_completion:\n",
    "        print(f\"Very strange output: completion_len>=length_completion: {completion_len}>={length_completion}. Skipping\")\n",
    "        print(completion_trimmed)\n",
    "        return None, None, None\n",
    "    if context_len>=length_context:\n",
    "        print(f\"Very strange output: context_len>=length_context: {context_len}>={length_context}. Skipping\")\n",
    "        print(completion_trimmed)\n",
    "        return None, None, None\n",
    "    # if 2*completion_len>=length_completion:\n",
    "    #     print(f\"Look here: 2*completion_len>=length_completion: 2*{completion_len}>={length_completion}. Not skipping\")\n",
    "    #     print(completion_trimmed)\n",
    "    # if 2*context_len>=length_context:\n",
    "    #     print(f\"Look here: 2*context_len>=length_context: 2*{context_len}>={length_context}. Not skipping\")\n",
    "    #     print(completion_trimmed)\n",
    "    \n",
    "        \n",
    "    # print(f\"context_len: {context_len}, completion_len: {completion_len}\")\n",
    "    # # if completion_len>1000:\n",
    "    #     # print(completion_trimmed)\n",
    "    # print(f\"length_context:{length_context}, length_completion:{length_completion}, len(context_trimmed): {len(context_trimmed)}, len(completion_trimmed): {len(completion_trimmed)}\")\n",
    "    # print(f\"context_trim_idx:{context_trim_idx}, len(context): {len(context)}, compl_trim_idx:{compl_trim_idx}, len(completion): {len(completion)}\")\n",
    "    if len(completion_trimmed)<completion_len:\n",
    "        print(completion_trimmed)\n",
    "    diff_tokens = 0\n",
    "   \n",
    "    if (context_len+completion_len > max_len_tokens):\n",
    "        diff_tokens = context_len+completion_len - max_len_tokens\n",
    "        if cut_context:\n",
    "            res_input['input_ids'] =res_input['input_ids'][:, diff_tokens:]\n",
    "            res_input['attention_mask'] =res_input['attention_mask'][:,diff_tokens:]\n",
    "            context_len = context_len - diff_tokens\n",
    "        else:\n",
    "            res_input['input_ids'] =res_input['input_ids'][:, :-diff_tokens]\n",
    "            res_input['attention_mask'] =res_input['attention_mask'][:,:-diff_tokens]\n",
    "            completion_len = completion_len - diff_tokens\n",
    "            \n",
    "    \n",
    "    return res_input, context_len, completion_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81289799-1622-4ae7-bcb4-efbe6be0acfe",
   "metadata": {},
   "source": [
    "First cut of completion\n",
    "First cut of context\n",
    "context_len: 1222, completion_len: \n",
    "length_context:4501, length_completion:1501, len(context_trimmed): 4474, len(completion_trimmed): 1469\n",
    "context_trim_idx:995525, len(context): 1000000, compl_trim_idx:1469, len(completion): 20843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4810549b-4dd8-42bf-bdb5-24ccd9209540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "def live_plot(data1, data2, data3, data4, figsize=(10, 20), title1='Train loss on whole sequence (EMA)', title2='Train loss on completion (EMA)',\n",
    "             title3='Val loss on whole sequence (EMA)', title4='Val loss on completion (EMA)'):\n",
    "    clear_output(wait=True)\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=figsize)  # Creates 4 subplots vertically aligned\n",
    "\n",
    "    # Plotting data1 on the first subplot\n",
    "    ax1.plot(data1, marker='o')\n",
    "    ax1.set_title(title1)\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plotting data2 on the second subplot\n",
    "    ax2.plot(data2, marker='o')\n",
    "    ax2.set_title(title2)\n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Plotting data1 on the first subplot\n",
    "    ax3.plot(data3, marker='o')\n",
    "    ax3.set_title(title3)\n",
    "    ax3.set_xlabel('Iteration')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Plotting data2 on the second subplot\n",
    "    ax4.plot(data4, marker='o')\n",
    "    ax4.set_title(title4)\n",
    "    ax4.set_xlabel('Iteration')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5d40f5d-81b7-4bbf-994e-93d522e757a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9833e653d65e4bf7ae9a6bc65e686c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80752bf308f4407392b572425d16ce91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda3353a058e4548a189af035a6cf283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class HFDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.hf_dataset[idx]\n",
    "\n",
    "dataset = load_dataset(\"JetBrains-Research/context-py-train\", \"path_distance_relevant\", cache_dir='/mnt/data2/shared-data/lca/hf_cache2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37a89c80-1e96-4e75-9bd3-ade89037d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_dataset(num_points = 128):\n",
    "    curr_num_points = 0\n",
    "    validation_repos = set()\n",
    "    train_repos = list(set(dataset['train']['repo']))\n",
    "    cnt = Counter(dataset['train']['repo'])\n",
    "    val_indices_list = []\n",
    "    while curr_num_points < num_points:  \n",
    "        curr_repo = random.choice(train_repos)\n",
    "        num_points_to_add = min(5, num_points-curr_num_points, cnt[curr_repo])\n",
    "        indices = [i for i, value in enumerate(dataset['train']['repo']) if value == curr_repo]\n",
    "        val_indices_list.extend(random.sample(indices, num_points_to_add))\n",
    "        validation_repos.add(curr_repo)\n",
    "        train_repos.remove(curr_repo)\n",
    "        curr_num_points = curr_num_points + num_points_to_add\n",
    "    val_dataset = dataset['train'].select(val_indices_list)\n",
    "    return val_dataset, validation_repos, train_repos, val_indices_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20e24acc-f7c0-44ba-bdbc-11dc5c4404db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset, validation_repos, train_repos, val_indices_list = get_val_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a695efb-16fa-45e2-8f8e-810d80ae4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_val_dataset(\"v2_all\", val_dataset, validation_repos, val_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa847dc1-6eb9-41d7-9d0c-a1d9cd753b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset, validation_repos, val_indices_list = read_val_dataset(\"v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bb680bc-6c20-4283-9d75-a2b7eb66fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter function to split based on repositories\n",
    "def is_validation(entry):\n",
    "    return entry['repo'] in validation_repos\n",
    "\n",
    "def is_train(entry):\n",
    "    return entry['repo'] not in validation_repos\n",
    "\n",
    "def get_new_ema(curr_loss, prev_ema):\n",
    "    return (curr_loss - prev_ema)*EMA_ALPHA + prev_ema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2aee3b1-89a2-4fe3-9f62-dc5fd53359e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train'].filter(is_train)\n",
    "torch_train_dataset = HFDataset(train_dataset)\n",
    "train_loader = DataLoader(torch_train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "torch_val_dataset = HFDataset(val_dataset)\n",
    "val_loader = DataLoader(torch_val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd565f3b-fbde-4288-a5b5-c05c5a3a632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d5f5d-64b1-4ae1-a131-161aed9d974c",
   "metadata": {},
   "source": [
    "Accelerator():\n",
    "\n",
    "NotImplementedError: Using RTX 4000 series doesn't support faster communication broadband via P2P or IB. Please set `NCCL_P2P_DISABLE=\"1\"` and `NCCL_IB_DISABLE=\"1\" or use `accelerate launch` which will do this automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b4c70e9-d96a-48d2-a667-c67a13867a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCCL_P2P_DISABLE=\"1\"\n",
    "NCCL_IB_DISABLE=\"1\"\n",
    "# os.environ[\"ACCELERATE_CONFIG\"] = \"accelerate_config.json\"\n",
    "# accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32788a55-0a0d-47d6-ace3-30e6a7a9766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator.state.process_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3cde4e8-9056-4b22-99b3-1172a940ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f9789e8-4651-4423-8b5a-5443d75cf367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate config 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e697dd06-d2cb-498a-9dc7-00461da8af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NCCL_P2P_DISABLE'] = \"1\"\n",
    "os.environ['NCCL_IB_DISABLE'] = \"1\"\n",
    "\n",
    "devices_to_train_str = ','.join(str(x) for x in DEVICES_TO_TRAIN)\n",
    "# CHANGED\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"no\", gradient_accumulation_steps=ACCUM_STEPS_NUM)\n",
    "model, optimizer, train_loader, val_loader = accelerator.prepare(model, optimizer, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9978777a-56b2-4c8a-a151-1f5926fe5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13275\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c0e164e-9b77-4983-acbd-033ee630cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # hf_path='JetBrains-Research/context-py-train',\n",
    "#         hf_config=None,\n",
    "#         cache_dir='/mnt/data2/shared-data/lca/hf_cache2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4026d3b6-7161-42fc-b5c6-d0d0e827c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validation_step(model, val_losses_whole_input, val_losses_completion, val_loss_whole_input_ema, val_loss_completion_ema):\n",
    "    model.eval()  \n",
    "    with accelerator.autocast(), torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        total_val_tokens = 0\n",
    "        curr_val_loss_whole_input=[]\n",
    "        curr_val_loss_completion=[]\n",
    "        val_loader_iter = iter(val_loader)  # Create an iterator for the validation data\n",
    "\n",
    "        for idx, val_hf_dp in enumerate(val_loader_iter):\n",
    "\n",
    "            val_dp = DatapointComposed.from_hf_datapoint(val_hf_dp)\n",
    "            val_inputs, val_context_len, val_completion_len = compose_input_sequence(val_dp, CONTEXT_MAX_LEN_TOKENS, tokenizer, 0.75)\n",
    "            if val_inputs is not None:\n",
    "                val_inputs = val_inputs.to(model.device)\n",
    "                val_outputs = model.forward(**val_inputs)\n",
    "                val_logits_size = val_outputs['logits'].size(-1)\n",
    "                val_loss_completion = criterion(val_outputs['logits'].view(-1, val_outputs['logits'].size(-1))[-val_completion_len:-1, :], val_inputs['input_ids'].view(-1)[-val_completion_len+1:])\n",
    "                val_detached_logits = val_outputs['logits'].detach()\n",
    "                \n",
    "                # for 0 output logit we have corresponding 1st input\n",
    "                val_loss_whole_input = criterion(val_detached_logits.view(-1, val_detached_logits.size(-1))[:-1, :], val_inputs['input_ids'].view(-1)[1:]) \n",
    "                curr_val_loss_whole_input.append(val_loss_whole_input.item())\n",
    "                curr_val_loss_completion.append(val_loss_completion.item())\n",
    "                \n",
    "        #calculate losses\n",
    "        val_losses_whole_input.append(sum(curr_val_loss_whole_input)/len(curr_val_loss_whole_input))\n",
    "        val_losses_completion.append(sum(curr_val_loss_completion)/len(curr_val_loss_completion))\n",
    "        val_loss_whole_input = val_losses_whole_input[-1]\n",
    "        val_loss_completion = val_losses_completion[-1]\n",
    "        \n",
    "        if len(val_losses_whole_input)>EMA_PERIOD:\n",
    "            val_loss_whole_input_ema.append(get_new_ema(val_losses_whole_input[-1],val_loss_whole_input_ema[-1]))\n",
    "            val_loss_completion_ema.append(get_new_ema(val_losses_completion[-1],val_loss_completion_ema[-1]))      \n",
    "        elif len(val_losses_whole_input)==EMA_PERIOD:\n",
    "            val_loss_whole_input_ema.append(np.mean(val_losses_whole_input))\n",
    "            val_loss_completion_ema.append(np.mean(val_losses_completion))\n",
    "        if len(val_losses_whole_input)>=EMA_PERIOD:    \n",
    "            val_loss_whole_input_ema_val = val_loss_whole_input_ema[-1]\n",
    "            val_loss_completion_ema_val = val_loss_completion_ema[-1]\n",
    "            \n",
    "        else:\n",
    "            val_loss_whole_input_ema_val=-1\n",
    "            val_loss_completion_ema_val=-1\n",
    "    \n",
    "    model.train()  # Set the model back to train mode\n",
    "    return val_loss_whole_input, val_loss_completion, val_loss_whole_input_ema_val, val_loss_completion_ema_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "025f4f6b-a592-4732-a485-91052d6fe5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_loss(model):\n",
    "    model.eval()  \n",
    "    with accelerator.autocast(), torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        total_val_tokens = 0\n",
    "        curr_val_loss_whole_input=[]\n",
    "        curr_val_loss_completion=[]\n",
    "        val_loader_iter = iter(val_loader)  # Create an iterator for the validation data\n",
    "\n",
    "        for idx, val_hf_dp in enumerate(val_loader_iter):\n",
    "\n",
    "            val_dp = DatapointComposed.from_hf_datapoint(val_hf_dp)\n",
    "            val_inputs, val_context_len, val_completion_len = compose_input_sequence(val_dp, CONTEXT_MAX_LEN_TOKENS, tokenizer, 0.75)\n",
    "            if val_inputs is not None:\n",
    "                val_inputs = val_inputs.to(model.device)\n",
    "                val_outputs = model.forward(**val_inputs)\n",
    "                val_logits_size = val_outputs['logits'].size(-1)\n",
    "                val_loss_completion = criterion(val_outputs['logits'].view(-1, val_outputs['logits'].size(-1))[-val_completion_len:-1, :], val_inputs['input_ids'].view(-1)[-val_completion_len+1:])\n",
    "                val_detached_logits = val_outputs['logits'].detach()\n",
    "                \n",
    "                # for 0 output logit we have corresponding 1st input\n",
    "                val_loss_whole_input = criterion(val_detached_logits.view(-1, val_detached_logits.size(-1))[:-1, :], val_inputs['input_ids'].view(-1)[1:]) \n",
    "                curr_val_loss_whole_input.append(val_loss_whole_input.item())\n",
    "                curr_val_loss_completion.append(val_loss_completion.item())\n",
    "                \n",
    "        return sum(curr_val_loss_whole_input)/len(curr_val_loss_whole_input), sum(curr_val_loss_completion)/len(curr_val_loss_completion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4380e01-1047-4e37-821c-ca5fb95b167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config={\n",
    "    \"ACCUM_STEPS_NUM\": ACCUM_STEPS_NUM,\n",
    "    \"VALIDATION_PERIOD\": VALIDATION_PERIOD,\n",
    "    \"CONTEXT_MAX_LEN_TOKENS\": CONTEXT_MAX_LEN_TOKENS,\n",
    "    \"EMA_PERIOD\": EMA_PERIOD, \n",
    "    \"VAL_DATASET_SIZE\": 128,\n",
    "    \"ADAMW_MAX_LR\": 5e-4, \n",
    "    \"ADAMW_weight_decay\":0.01,\n",
    "    \"warmup\":200,\n",
    "    # \"Cosine_T_max\":50, \n",
    "    # \"Cosine_eta_min\":1e-6,\n",
    "    # \"num_GPU_cards\": 2,\n",
    "    \"DEVICES_TO_TRAIN\":DEVICES_TO_TRAIN,\n",
    "    \"DeepSpeed_plugin\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e1a0b92-342e-47c6-bb03-4b00d0b6fac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACCUM_STEPS_NUM': 64, 'VALIDATION_PERIOD': 10, 'CONTEXT_MAX_LEN_TOKENS': 2000, 'EMA_PERIOD': 40, 'VAL_DATASET_SIZE': 128, 'ADAMW_MAX_LR': 0.0005, 'ADAMW_weight_decay': 0.01, 'warmup': 200, 'DEVICES_TO_TRAIN': [3], 'DeepSpeed_plugin': False}\n"
     ]
    }
   ],
   "source": [
    "print(wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3720271a-0933-4125-89cf-513a838adae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/blatova/lca-solvers/finetune/wandb/run-20240711_200703-rplnx0nz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/org12345/finetuning_with_lora/runs/rplnx0nz' target=\"_blank\">fine-glitter-52</a></strong> to <a href='https://wandb.ai/org12345/finetuning_with_lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/org12345/finetuning_with_lora' target=\"_blank\">https://wandb.ai/org12345/finetuning_with_lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/org12345/finetuning_with_lora/runs/rplnx0nz' target=\"_blank\">https://wandb.ai/org12345/finetuning_with_lora/runs/rplnx0nz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/org12345/finetuning_with_lora/runs/rplnx0nz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f30520d2f80>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"finetuning_with_lora\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config=wandb_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61f21d6b-2933-4a53-843e-9239fbd10566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/accelerate/blob/main/examples/by_feature/deepspeed_with_config_support.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "311a5c2a-1d2e-490c-a2ec-616c1c0a73d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer._is_accelerate_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67b1e354-d0d7-4747-98d9-3a5e918eb7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02341fec-f30d-4b22-a329-a425d691bd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACCUM_STEPS_NUM*50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50878b0-5465-4a56-bf0d-b1c948016f19",
   "metadata": {},
   "source": [
    "https://pytorch.org/torchtune/stable/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#transformers.get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1c163-6e5a-4218-abf1-19974d6dec44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55a3fe06-dc29-42ee-96e6-d449c398e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    torch.cuda.empty_cache()\n",
    "    total_whole_input_loss = 0\n",
    "    total_completion_loss = 0\n",
    "    total_tokens = 0\n",
    "    total_completion_tokens = 0\n",
    "    train_losses_whole_input = []\n",
    "    curr_train_loss_whole_input = []\n",
    "    train_losses_completion = []\n",
    "    curr_train_loss_completion = []\n",
    "    \n",
    "    val_losses_whole_input = []\n",
    "    val_losses_completion = []\n",
    "    \n",
    "    \n",
    "    train_loss_whole_input_ema = []\n",
    "    train_loss_completion_ema = []\n",
    "    val_loss_whole_input_ema = []\n",
    "    val_loss_completion_ema = []\n",
    "    wandb_step=1\n",
    "    \n",
    "    val_loss_whole_input, val_loss_completion = get_val_loss(model)\n",
    "    # CHANGED\n",
    "    checkpoint_num = 0\n",
    "    save_checkpoint(checkpoint_num, model, val_loss_whole_input, val_loss_completion, val_dataset, suffix=SUFFIX)\n",
    "    checkpoint_num=checkpoint_num+1\n",
    "    scheduler_type=0\n",
    "    scheduler_step=1\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loader_iter = iter(train_loader)\n",
    "    \n",
    "        for idx, hf_dp in enumerate(train_loader_iter):\n",
    "            # torch.cuda.empty_cache()\n",
    "            # if idx > 20:\n",
    "            #     break\n",
    "        \n",
    "            dp = DatapointComposed.from_hf_datapoint(hf_dp)\n",
    "            inputs, context_len, completion_len = compose_input_sequence(dp, CONTEXT_MAX_LEN_TOKENS, tokenizer, 0.75)\n",
    "            if inputs is not None:\n",
    "                assert abs(completion_len+context_len-inputs['input_ids'].shape[1])<2\n",
    "                # full_len = inputs['input_ids'].shape[1]\n",
    "                inputs = inputs.to(model.device)\n",
    "                outputs = model.forward(**inputs)\n",
    "                logits_size = outputs['logits'].size(-1)\n",
    "                loss_completion = criterion(outputs['logits'].view(-1, outputs['logits'].size(-1))[-completion_len:-1, :], inputs['input_ids'].view(-1)[-completion_len+1:])\n",
    "                # CHANGED\n",
    "                accelerator.backward(loss_completion)\n",
    "                # loss_completion.backward()\n",
    "                detached_logits = outputs['logits'].detach()\n",
    "                \n",
    "                # for 0 output logit we have corresponding 1st input\n",
    "                loss_whole_input = criterion(detached_logits.view(-1, detached_logits.size(-1))[:-1, :], inputs['input_ids'].view(-1)[1:]) \n",
    "                \n",
    "                # calculate losses and backprop\n",
    "                if (idx+1) % ACCUM_STEPS_NUM ==0:\n",
    "                    train_losses_whole_input.append(sum(curr_train_loss_whole_input)/len(curr_train_loss_whole_input))\n",
    "                    train_losses_completion.append(sum(curr_train_loss_completion)/len(curr_train_loss_completion))\n",
    "\n",
    "                    train_loss_whole_input_ema_val = 0\n",
    "                    train_loss_completion_ema_val = 0\n",
    "                    if len(train_losses_whole_input)>EMA_PERIOD:\n",
    "                        train_loss_whole_input_ema_val = get_new_ema(train_losses_whole_input[-1],train_loss_whole_input_ema[-1])\n",
    "                        train_loss_completion_ema_val = get_new_ema(train_losses_completion[-1],train_loss_completion_ema[-1])\n",
    "                    elif len(train_losses_whole_input)==EMA_PERIOD:\n",
    "                        train_loss_whole_input_ema_val = np.mean(train_losses_whole_input)\n",
    "                        train_loss_completion_ema_val = np.mean(train_losses_completion)\n",
    "                    else:\n",
    "                        train_loss_whole_input_ema_val = -1\n",
    "                        train_loss_whole_completion_ema_val = -1\n",
    "\n",
    "                    if len(train_losses_whole_input)>=EMA_PERIOD:\n",
    "                        train_loss_whole_input_ema.append(train_loss_whole_input_ema_val)\n",
    "                        train_loss_completion_ema.append(train_loss_completion_ema_val)\n",
    "        \n",
    "                    curr_train_loss_whole_input = []\n",
    "                    curr_train_loss_completion=[]\n",
    "                    for param in model.parameters():\n",
    "                        if param.requires_grad:\n",
    "                            if param.grad is not None:\n",
    "                                param.grad /= ACCUM_STEPS_NUM\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()      \n",
    "                    \n",
    "                    if scheduler_step < WARMUP_STEPS:\n",
    "                        scheduler_warmup.step()\n",
    "                        curr_lr = scheduler_warmup.get_lr()[0]\n",
    "                        scheduler_type=0\n",
    "                    else:\n",
    "                        scheduler_cosine.step()\n",
    "                        curr_lr = scheduler_cosine.get_lr()[0]\n",
    "                        scheduler_type=1\n",
    "                    if train_loss_whole_input_ema_val>0:    \n",
    "                        wandb.log({\"scheduler_lrr\": curr_lr, \"scheduler_type\": scheduler_type, \"scheduler_step\": scheduler_step, \n",
    "                                   \"epoch\":epoch,\n",
    "                                  \"train_loss_whole_input\": train_losses_whole_input[-1], \n",
    "                                  \"train_loss_completion\": train_losses_completion[-1],\n",
    "                                  \"train_loss_whole_input_ema\": train_loss_whole_input_ema_val, \n",
    "                                  \"train_loss_completion_ema\": train_loss_completion_ema_val}, step = wandb_step)\n",
    "                    else:\n",
    "                       wandb.log({\"scheduler_lrr\": curr_lr, \"scheduler_type\": scheduler_type, \"scheduler_step\": scheduler_step, \n",
    "                                   \"epoch\":epoch,\n",
    "                                  \"train_loss_whole_input\": train_losses_whole_input[-1], \n",
    "                                  \"train_loss_completion\": train_losses_completion[-1]}, step = wandb_step) \n",
    "                    scheduler_step = scheduler_step+1\n",
    "        \n",
    "                #we could set some other parameter here\n",
    "                # if ((idx+1) % ACCUM_STEPS_NUM*VALIDATION_PERIOD) ==0:\n",
    "                if ((idx+1) % ACCUM_STEPS_NUM) ==0:\n",
    "                    val_loss_whole_input_val, val_loss_completion_val, val_loss_whole_input_ema_val, val_loss_completion_ema_val = make_validation_step(model, val_losses_whole_input, val_losses_completion, val_loss_whole_input_ema, val_loss_completion_ema)\n",
    "                    if val_loss_whole_input_ema_val>0:\n",
    "                        wandb.log({\"val_loss_whole_input\":val_loss_whole_input_val, \"val_loss_completion\": val_loss_completion_val, \n",
    "                                  \"val_loss_whole_input_ema\": val_loss_whole_input_ema_val, \n",
    "                                   \"val_loss_completion_ema\": val_loss_completion_ema_val}, step = wandb_step)\n",
    "                    else:\n",
    "                        wandb.log({\"val_loss_whole_input\":val_loss_whole_input_val, \"val_loss_completion\": val_loss_completion_val}, step = wandb_step)\n",
    "\n",
    "                if ((idx+1) % ACCUM_STEPS_NUM) ==0:\n",
    "                    wandb_step = wandb_step+1\n",
    "                    \n",
    "        \n",
    "                if ((idx+1) % (ACCUM_STEPS_NUM*50)) ==0:\n",
    "                    # CHANGED\n",
    "                    save_checkpoint(checkpoint_num, model, val_losses_whole_input[-1], val_losses_completion[-1], val_dataset, suffix=SUFFIX)\n",
    "                    checkpoint_num=checkpoint_num+1\n",
    "                    \n",
    "                    \n",
    "            \n",
    "                # if (idx+1) % (ACCUM_STEPS_NUM*5) ==0:\n",
    "                #     if (len(train_loss_whole_input_ema)>2) and (len(val_loss_whole_input_ema)>2):\n",
    "                #         live_plot(train_loss_whole_input_ema, train_loss_completion_ema, val_loss_whole_input_ema, val_loss_completion_ema)\n",
    "                #         print(f\"Plotted: {(idx+1)} steps\")\n",
    "        \n",
    "                    \n",
    "                \n",
    "                curr_train_loss_whole_input.append(loss_whole_input.item())\n",
    "                curr_train_loss_completion.append(loss_completion.item())\n",
    "                total_whole_input_loss += loss_whole_input.item() * (inputs['input_ids'].size(0)-1)  # Accumulate scaled loss\n",
    "                total_completion_loss += loss_completion.item() * (completion_len -1)  # Accumulate scaled loss\n",
    "                total_tokens += len(inputs['input_ids']) - 1  # Count tokens processed\n",
    "                total_completion_tokens += completion_len - 1\n",
    "    \n",
    "            \n",
    "    \n",
    "            \n",
    "            \n",
    "    avg_loss = total_whole_input_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)  # Calculate perplexity as exp of the average loss\n",
    "    \n",
    "    avg_completion_loss = total_completion_loss / total_completion_tokens\n",
    "    perplexity = math.exp(avg_completion_loss)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "763240d7-6dcf-4e6b-bbaf-429039481ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log({\"scheduler_lrr\":scheduler.get_lr()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45dfc756-1d2f-43b9-91d2-a6df4cfd107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr4e-5_old_scheduler_with_warmup_3_epochs_relevant\n"
     ]
    }
   ],
   "source": [
    "print(SUFFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f266fd-9087-47d0-8965-3124d85c7329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f9c14984464eb884b9448e14cc9ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf27e9-df21-4482-a12d-06a43ed2bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc9468-c218-426a-a6cf-deec013acdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384ac10-9eb7-491a-88b2-9ad828656aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e6817-0146-45a9-8b13-94b9532f6295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11fa5794-c44b-4f0a-806e-28815c9d8761",
   "metadata": {},
   "source": [
    "First cut of completion\n",
    "First cut of context\n",
    "context_len: 1222, completion_len: 2548\n",
    "length_context:4501, length_completion:1501, len(context_trimmed): 4474, len(completion_trimmed): 1469\n",
    "context_trim_idx:995525, len(context): 1000000, compl_trim_idx:1469, len(completion): 20843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38df1b-cafb-4bc4-89a9-4645dc36be88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251787c-7ee5-450e-b557-1c992acc2c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b7c1f-9a43-4a2d-b0dd-e19906c056ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c5091-7df0-4e2a-af2d-7d6530f40236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f793e-b0b9-445f-a6d5-b2d5c4b7ae36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3ee0c-245e-471a-a157-f8e5125ca2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b6fb2-b2d4-4573-9c76-610844f66e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce60b1-8b6f-4da8-b660-98caaec95580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3b0db-1087-4926-ad9e-d2a041cdd6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
